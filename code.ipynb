{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import random \n",
    "import shutil \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import optim \n",
    "import torchvision \n",
    "import torchmetrics\n",
    "from torchvision import models \n",
    "from torchvision import transforms \n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and testing dataset class\n",
    "class DFD_dataset(Dataset):\n",
    "    def __init__(self, img_path_list, true_class_list, transforms=None):\n",
    "        imgs = []\n",
    "        for i in range(len(img_path_list)):\n",
    "            imgs.append((img_path_list[i], true_class_list[i]))\n",
    "        \n",
    "        self.imgs = imgs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.imgs[index]\n",
    "        img = Image.open(img_path)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "test_tranform = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(img_path_list, true_class_list):\n",
    "    # init dataloader\n",
    "    test_data = DFD_dataset(img_path_list=img_path_list, true_class_list=true_class_list, transforms=test_tranform)\n",
    "    for i in range(len(test_data)):\n",
    "        img_index.append(test_data.imgs[i][0])\n",
    "    \n",
    "    testloader = dataloader.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # load model\n",
    "    # please keep saved model file in the same directory with this .ipynb file\n",
    "    device = torch.device('cpu')\n",
    "    network = models.inception_v3(pretrained = True)\n",
    "    num_fc_in = network.fc.in_features\n",
    "    network.fc = nn.Linear(num_fc_in, 2)\n",
    "    network.load_state_dict(torch.load('./best.pt', map_location=device))\n",
    "\n",
    "    # calculate accuracies\n",
    "    network.eval()\n",
    "    acc = 0\n",
    "    recall = 0\n",
    "    precision = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    TP = 0 \n",
    "    FP = 0\n",
    "    _d = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = network(images)\n",
    "            for index,label in enumerate(labels.numpy()):\n",
    "                if label:\n",
    "                    if outputs.argmax(1).numpy()[index]:\n",
    "                        TP+=1\n",
    "                    else:\n",
    "                        FN+=1\n",
    "                else:\n",
    "                    if outputs.argmax(1).numpy()[index]:\n",
    "                        FP+=1\n",
    "                    else:\n",
    "                        TN+=1\n",
    "    acc = (TP + TN) / (TP + TN + FN + FP)\n",
    "    recall = (TP) / (TP + FN)\n",
    "    precision = (TP) / (TP + FP)   \n",
    "    return acc, recall, precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Code\n",
    "img_path_list = []\n",
    "true_class_list = []\n",
    "    \n",
    "acc, recall, precision = test(img_path_list, true_class_list)\n",
    "print(acc)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary dependencies\n",
    "import os \n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import random \n",
    "import shutil \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch import optim \n",
    "from torch.optim import lr_scheduler \n",
    "import torchvision \n",
    "from torchvision import models \n",
    "from torchvision import transforms \n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import dataloader \n",
    "from torch.utils.tensorboard import SummaryWriter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- InceptionV3 82 Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to keep this .ipynb file in the same directory as the images folder\n",
    "# divede the images provided into training and validation set (8:2)\n",
    "# create directories\n",
    "current_path = os.getcwd()\n",
    "image_path = current_path + '/images/'\n",
    "\n",
    "train_path = current_path + '/train/'\n",
    "val_path = current_path + '/val/'\n",
    "\n",
    "train_fake_path = train_path + 'fake/'\n",
    "train_real_path = train_path + 'real/'\n",
    "\n",
    "val_fake_path = val_path + 'fake/'\n",
    "val_real_path = val_path + 'real/'\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    os.makedirs(train_path)\n",
    "if not os.path.exists(val_path):\n",
    "    os.makedirs(val_path)\n",
    "    \n",
    "if not os.path.exists(train_fake_path):\n",
    "    os.makedirs(train_fake_path)\n",
    "if not os.path.exists(train_real_path):\n",
    "    os.makedirs(train_real_path)\n",
    "\n",
    "if not os.path.exists(val_fake_path):\n",
    "    os.makedirs(val_fake_path)\n",
    "if not os.path.exists(val_real_path):\n",
    "    os.makedirs(val_real_path)\n",
    "\n",
    "# distribute 12000 images into different folders\n",
    "train_fake_num = 0\n",
    "train_real_num = 0\n",
    "\n",
    "val_fake_num = 0\n",
    "val_real_num = 0\n",
    "\n",
    "test_fake_num = 0\n",
    "test_real_num = 0\n",
    "\n",
    "# loop through images folder\n",
    "for rootpath, dirnames, filenames in os.walk(image_path):\n",
    "    for dirname in dirnames:\n",
    "        if dirname == 'fake_deepfake':\n",
    "            # generate 800 random number in the range [0, 3999] to represent those go to val\n",
    "            # force pseudorandom split\n",
    "            random.seed(4487)\n",
    "            val_index = random.sample(range(0, 4000), 800)\n",
    "            # directory full path\n",
    "            image_folder = rootpath + dirname + '/'\n",
    "            # loop all images in fake_deepfake folder\n",
    "            imgfiles = os.listdir(image_folder)\n",
    "            for imgfile in imgfiles:\n",
    "                srcpath = image_folder + imgfile\n",
    "                index = int(imgfile.split('.')[0])\n",
    "                if index in val_index:\n",
    "                    newname = str(val_fake_num) + '.png'\n",
    "                    dstpath = val_fake_path + newname\n",
    "                    shutil.copyfile(srcpath, dstpath)\n",
    "                    val_fake_num += 1\n",
    "                else:\n",
    "                    newname = str(train_fake_num) + '.png'\n",
    "                    dstpath = train_fake_path + newname\n",
    "                    shutil.copyfile(srcpath, dstpath)\n",
    "                    train_fake_num += 1\n",
    "            print('done')\n",
    "        elif dirname == 'fake_face2face':\n",
    "            # generate 800 random number in the range [0, 3999] to represent those go to val\n",
    "            # force pseudorandom split\n",
    "            random.seed(4486)\n",
    "            val_index = random.sample(range(0, 4000), 800)\n",
    "            # directory full path\n",
    "            image_folder = rootpath + dirname + '/'\n",
    "            # loop all images in fake_face2face folder\n",
    "            imgfiles = os.listdir(image_folder)\n",
    "            for imgfile in imgfiles:\n",
    "                srcpath = image_folder + imgfile\n",
    "                index = int(imgfile.split('.')[0])\n",
    "                if index in val_index:\n",
    "                    newname = str(val_fake_num) + '.png'\n",
    "                    dstpath = val_fake_path + newname\n",
    "                    shutil.copyfile(srcpath, dstpath)\n",
    "                    val_fake_num += 1\n",
    "                else:\n",
    "                    newname = str(train_fake_num) + '.png'\n",
    "                    dstpath = train_fake_path + newname\n",
    "                    shutil.copyfile(srcpath, dstpath)\n",
    "                    train_fake_num += 1\n",
    "            print('done')\n",
    "        elif dirname == 'real':\n",
    "            # generate 800 random number in the range [0, 3999] to represent those go to val\n",
    "            # force pseudorandom split\n",
    "            random.seed(4485)\n",
    "            val_index = random.sample(range(0, 4000), 800)\n",
    "            # directory full path\n",
    "            image_folder = rootpath + dirname + '/'\n",
    "            # loop all images in real folder\n",
    "            imgfiles = os.listdir(image_folder)\n",
    "            for imgfile in imgfiles:\n",
    "                srcpath = image_folder + imgfile\n",
    "                index = int(imgfile.split('.')[0])\n",
    "                if index in val_index:\n",
    "                    newname = str(val_real_num) + '.png'\n",
    "                    dstpath = val_real_path + newname\n",
    "                    shutil.copyfile(srcpath, dstpath)\n",
    "                    val_real_num += 1\n",
    "                else:\n",
    "                    newname = str(train_real_num) + '.png'\n",
    "                    dstpath = train_real_path + newname\n",
    "                    shutil.copyfile(srcpath, dstpath)\n",
    "                    train_real_num += 1\n",
    "            print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and validation dataset class\n",
    "class DFD_dataset(Dataset):\n",
    "    def __init__(self, img_path, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.img_path = img_path\n",
    "        self.real_dir = img_path + 'real'\n",
    "        self.fake_dir = img_path + 'fake'\n",
    "        self.real_num = len(os.listdir(self.real_dir))\n",
    "        self.fake_num = len(os.listdir(self.fake_dir))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.real_num + self.fake_num\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < self.real_num:\n",
    "            label = 1\n",
    "            img = Image.open(self.real_dir + '/' + str(index) + '.png')\n",
    "        else:\n",
    "            label = 0\n",
    "            img = Image.open(self.fake_dir + '/' + str(index - self.real_num) + '.png')\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define img transformers and create dataLoaders\n",
    "train_tranform = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10, expand=False, fill=None),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_tranform = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = DFD_dataset(train_path, transforms=train_tranform)\n",
    "trainloader = dataloader.DataLoader(train_data, batch_size=60, shuffle=True)\n",
    "\n",
    "val_data = DFD_dataset(val_path, transforms=val_tranform)\n",
    "valloader = dataloader.DataLoader(val_data, batch_size=60, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cuda is available, use GPU to accelerate training process\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# init network (pre-trained on ImageNet)\n",
    "network = models.inception_v3(pretrained=True)\n",
    "# input channels to fc\n",
    "num_fc_in = network.fc.in_features\n",
    "# change out features to 2 (fit our binary classification task)\n",
    "network.fc = nn.Linear(num_fc_in, 2)\n",
    "network = network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# set different learning rate for revised fc layer and previous layers\n",
    "# add weight decay (L2 Regularization)\n",
    "lr = 0.008 / 10\n",
    "fc_params = list(map(id, network.fc.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in fc_params, network.parameters())\n",
    "optimizer = optim.Adam([\n",
    "            {'params': base_params},\n",
    "            {'params': network.fc.parameters(), 'lr': lr * 10}],\n",
    "            lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
    "# learning rate decay function\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test function to calculate both training and val accuracy\n",
    "def test(network, loader, optimizer):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = network(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return round((float(100) * float(correct) / float(total)), 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define this function to save state_dict of each epoch\n",
    "def save_local(network, rootpath, epoch):\n",
    "    path = rootpath + '/' + str(epoch) + '.pt'\n",
    "    torch.save(network.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to set random seed for each epoch\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "epoch_num = 20\n",
    "# data recorders\n",
    "training_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "test_acc = []\n",
    "# change accordingly\n",
    "model_root_path = current_path + '/model_res/'\n",
    "networkInfo = 'Group02_InceptionV3'\n",
    "model_path = model_root_path + networkInfo\n",
    "# make dirs\n",
    "if not os.path.exists(model_root_path):\n",
    "    os.makedirs(model_root_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# force pseudorandom to generate 20 random seeds for reproduce\n",
    "random.seed(88)\n",
    "seeds = random.sample(range(0, 88), 20)\n",
    "\n",
    "# begin training\n",
    "for epoch in range(epoch_num):\n",
    "    network.train()\n",
    "    running_loss = 0.0\n",
    "    # set random seed for current epoch\n",
    "    set_seed(seeds[epoch])\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs, x = network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # calculate loss and accuracy\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    training_loss.append(epoch_loss)\n",
    "    train_acc.append(test(network, trainloader, optimizer))\n",
    "    val_acc.append(test(network, valloader, optimizer))\n",
    "    # whether to save current model\n",
    "    if epoch > 4:\n",
    "        save_local(network, model_path, epoch)\n",
    "    # print result of current epoch\n",
    "    print('Train Epoch: {}\\t Loss: {:.6f}'.format(epoch, epoch_loss))\n",
    "    # step forward the scheduler function\n",
    "    scheduler.step()\n",
    "\n",
    "# end training\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline\n",
    "# plot the traing loss v.s. epoch number\n",
    "\n",
    "# create dirs\n",
    "plot_root_path = current_path + '/viz/'\n",
    "plot_path = plot_root_path + networkInfo\n",
    "if not os.path.exists(plot_root_path):\n",
    "    os.makedirs(plot_root_path)\n",
    "if not os.path.exists(plot_path):\n",
    "    os.makedirs(plot_path)\n",
    "\n",
    "# plot\n",
    "epoch = list(range(20))\n",
    "training_avg = []\n",
    "cnt = 0.0\n",
    "counter = 0\n",
    "for i in training_loss:\n",
    "    counter += 1\n",
    "    cnt += i\n",
    "    training_avg.append(cnt / counter)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss for ' + networkInfo)\n",
    "plt.scatter(epoch, training_loss, c='b', marker='o', s=25)\n",
    "epoch_np = np.array(epoch)\n",
    "avg_np = np.array(training_avg)\n",
    "epoch_new = np.linspace(epoch_np.min(), epoch_np.max(), 300)\n",
    "avg_smooth = make_interp_spline(epoch_np, avg_np)(epoch_new)\n",
    "plt.plot(epoch_new, avg_smooth, c='r')\n",
    "my_x_ticks = np.arange(0, 20, 1)\n",
    "plt.xticks(my_x_ticks)\n",
    "# save plot\n",
    "plt.savefig(plot_path + '/' + 'training_loss.png', bbox_inches='tight', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracies v.s. epoch number\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train / Val Accuracy for ' + networkInfo)\n",
    "plt.plot(epoch, train_acc, marker='o', markersize=5, label='train')\n",
    "plt.plot(epoch, val_acc, c='orange', marker='o', markersize=5, label='val')\n",
    "plt.plot(epoch, test_acc, c='red', marker='o', markersize=5, label='test')\n",
    "my_x_ticks = np.arange(0, 20, 1)\n",
    "plt.xticks(my_x_ticks)\n",
    "plt.legend(loc='lower right')\n",
    "# save plot\n",
    "plt.savefig(plot_path + '/' + 'accuracy.png', bbox_inches='tight', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the maximum val_acc and corresponding num_of_epoch FYI\n",
    "print('The maximum val accuracy is: ', max(val_acc))\n",
    "print('It occurs when the number of training epoch equals to: ', val_acc.index(max(val_acc)))\n",
    "# save numpy array for possible use in the future\n",
    "numpy_root_path = current_path + '/numpy/'\n",
    "numpy_path = numpy_root_path + networkInfo\n",
    "if not os.path.exists(numpy_root_path):\n",
    "    os.makedirs(numpy_root_path)\n",
    "if not os.path.exists(numpy_path):\n",
    "    os.makedirs(numpy_path)\n",
    "\n",
    "np.save(numpy_path + '/' + 'training_loss.npy', np.array(training_loss))\n",
    "np.save(numpy_path + '/' + 'train_acc.npy', np.array(train_acc))\n",
    "np.save(numpy_path + '/' + 'val_acc.npy', np.array(val_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
